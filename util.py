import random
from typing import Optional, List, Dict, Tuple

import os
import joblib
import hashlib

import numpy as np
import pandas as pd
import torch
from matplotlib import pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder

from models import LSTMModel, LSTMPostMarkAtt, LSTMPostMarkAttCausalAd, LSTMCausalAd, TransformerModel
from models.time_mixer_adapter import TimeMixerAdapter

from collections import defaultdict
from dataSummary import analyze_data_distribution
from sklearn.preprocessing import PowerTransformer
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  # è‡ªåŠ¨é€‰æ‹©GPUæˆ–CPU


def set_seed(seed=2025):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# 2. åœ¨ set_seed å‡½æ•°ä¸‹æ–¹ï¼Œæ·»åŠ  time_features è¾…åŠ©å‡½æ•°:
def time_features(dates, freq='h'):
    """
    ç”Ÿæˆæ—¶é—´ç‰¹å¾ [Month, Day, Weekday, Hour]ï¼Œå½’ä¸€åŒ–åˆ° [-0.5, 0.5]
    ä¸“ä¾› TimeMixer ç­‰ Transformer ç±»æ¨¡å‹ä½¿ç”¨
    """
    if isinstance(dates, pd.Series):
        dates = dates.dt
    
    month = dates.month.values
    day = dates.day.values
    weekday = dates.weekday.values
    hour = dates.hour.values
    
    # å½’ä¸€åŒ–ç­–ç•¥ (TimeMixer æ ‡å‡†)
    f_month = (month - 1) / 11.0 - 0.5
    f_day = (day - 1) / 30.0 - 0.5
    f_weekday = weekday / 6.0 - 0.5
    f_hour = hour / 23.0 - 0.5
    
    return np.stack([f_month, f_day, f_weekday, f_hour], axis=1).astype(np.float32)
    

def batch_generator(data, batch_size, past_input_size=0, forward_input_size=0, shuffle=False):
    """
    é¡ºåºæ‰¹æ¬¡æ•°æ®ç”Ÿæˆå™¨ï¼ˆæ”¯æŒshuffleï¼‰
    å‚æ•°ï¼š
        data: æ•°æ®é›†
        batch_size: æ¯ä¸ªæ‰¹æ¬¡çš„æ ·æœ¬æ•°é‡
        past_input_size: å†å²è¾“å…¥ç‰¹å¾ç»´åº¦
        forward_input_size: æœªæ¥è¾“å…¥ç‰¹å¾ç»´åº¦
        shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®é¡ºåº
    """
    num_samples = len(data)
    indices = list(range(num_samples))

    if shuffle:
        random.shuffle(indices)  # æ‰“ä¹±ç´¢å¼•é¡ºåº

    start_idx = 0

    while start_idx < num_samples:
        # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„ç»“æŸç´¢å¼•
        end_idx = min(start_idx + batch_size, num_samples)

        # è·å–å½“å‰æ‰¹æ¬¡çš„ç´¢å¼•ï¼ˆå¯èƒ½æ˜¯æ‰“ä¹±åçš„ï¼‰
        batch_indices = indices[start_idx:end_idx]

        # åˆå§‹åŒ–å½“å‰æ‰¹æ¬¡çš„å­˜å‚¨åˆ—è¡¨
        batch_past = []
        batch_forward = []
        batch_target = []
        batch_ref_date = []
        batch_ids = []
        batch_names = []
        batch_original = [] # [æ–°å¢]
        batch_x_mark = [] # [æ–°å¢] å­˜å‚¨æ—¶é—´ç‰¹å¾

        # æŒ‰ç´¢å¼•é¡ºåºå¤„ç†å½“å‰æ‰¹æ¬¡çš„æ ·æœ¬
        for i in batch_indices:
            if forward_input_size > 0:
                if past_input_size > 0:
                    past, forward, target, ref_date, uid, name, original_target, x_mark = data[i]
                    batch_past.append(past)
                    batch_forward.append(forward)
                else:
                    _, forward, target, ref_date, uid, name, original_target, x_mark = data[i]
                    batch_forward.append(forward)
            else:
                past, _, target, ref_date, uid, name, original_target, x_mark = data[i]
                batch_past.append(past)

            batch_target.append(target)
            batch_ref_date.append(ref_date)
            batch_ids.append(uid)
            batch_names.append(name)
            batch_original.append(original_target) # [æ–°å¢]
            batch_x_mark.append(x_mark) # [æ–°å¢]

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        batch_past = np.array(batch_past) if batch_past else None
        batch_forward = np.array(batch_forward) if batch_forward else None
        batch_target = np.array(batch_target)
        batch_ref_date = np.array(batch_ref_date)
        batch_ids = np.array(batch_ids)
        batch_names = np.array(batch_names)
        batch_original = np.array(batch_original) # [æ–°å¢]
        # ... (numpy è½¬æ¢é€»è¾‘ä¿æŒä¸å˜) ...
        # æ–°å¢ x_mark è½¬æ¢
        batch_x_mark = np.array(batch_x_mark) if len(batch_x_mark) > 0 else None

        # æ›´æ–°ç´¢å¼•
        start_idx = end_idx

        # yield batch_past, batch_forward, batch_target, batch_ref_date, batch_ids, batch_names
        yield batch_past, batch_forward, batch_target, batch_ref_date, None, batch_names, batch_original, batch_x_mark # set batch_ids as none


# æ£€æŸ¥ç‚¹ï¼šéªŒè¯æ¨¡å‹åˆå§‹çŠ¶æ€
def model_fingerprint(model):
    return sum(p.sum().item() for p in model.parameters())


def create_model(args):
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    if args.model == 'LSTM':
        model = LSTMModel(
            input_size=args.input_size,
            embedding_size=args.embedding_size,
            hidden_size=args.hidden_size,
            num_layers=args.num_layers,
            step_forward=args.step_forward,
            past_input_size=args.past_input_size,
            forward_input_size=args.forward_input_size,
            dropout_rate=args.dropout_rate,
            num_ids=args.num_ids,
            id_embedding_size=args.id_embedding_size
        )
        use_adversarial = False
    elif args.model == 'LSTM_Attention':
        model = LSTMPostMarkAtt(
            input_size=args.input_size,
            embedding_size=args.embedding_size,
            hidden_size=args.hidden_size,
            num_layers=args.num_layers,
            step_forward=args.step_forward,
            att_head=args.attn_head,
            past_input_size=args.past_input_size,
            forward_input_size=args.forward_input_size,
            dropout_rate=args.dropout_rate
        )
        use_adversarial = False
    elif args.model == 'LSTMPostMarkAttCausalAd':
        model = LSTMPostMarkAttCausalAd(
            input_size=args.input_size,
            embedding_size=args.embedding_size,
            hidden_size_feat=args.hidden_size,
            hidden_size_target=args.hidden_size_target,
            num_layers=args.num_layers,
            step_forward=args.step_forward,
            attn_head_feat=args.attn_head,
            attn_head_target=args.attn_head_target,
            past_input_size=args.past_input_size,
            forward_input_size=args.forward_input_size,
            share_outNet=args.share_outNet,
            dropout_rate=args.dropout_rate
        )
        use_adversarial = True
    elif args.model == 'LSTMCausalAd':
        model = LSTMCausalAd(
            input_size=args.input_size,
            embedding_size=args.embedding_size,
            hidden_size_feat=args.hidden_size,
            hidden_size_target=args.hidden_size_target,
            num_layers=args.num_layers,
            step_forward=args.step_forward,
            past_input_size=args.past_input_size,
            forward_input_size=args.forward_input_size,
            share_outNet=args.share_outNet,
            dropout_rate=args.dropout_rate,
            num_ids=args.num_ids,
            id_embedding_size=args.id_embedding_size
        )
        use_adversarial = True
    elif args.model == 'TransformerModel':
        model = TransformerModel(
            input_size=args.input_size,
            embedding_size=args.embedding_size,
            hidden_size=args.hidden_size,
            num_layers=args.num_layers,
            step_forward=args.step_forward,
            past_input_size=args.past_input_size,
            forward_input_size=args.forward_input_size,
            dropout_rate=args.dropout_rate,
            num_ids=args.num_ids,
            id_embedding_size=args.id_embedding_size
        )
        use_adversarial = False
    elif args.model == 'TimeMixer':
        print("Initializing TimeMixer Adapter...")
        model = TimeMixerAdapter(args)
        use_adversarial = False
    else:
        raise ValueError('model name error')
    return model, use_adversarial


def compute_and_log_statistics(rmses):
    # è®¡ç®—ç»Ÿè®¡é‡
    stats = {
        "mean": float(np.mean(rmses)),
        "std": float(np.std(rmses)),
        "min": float(np.min(rmses)),
        "max": float(np.max(rmses)),
        "cv": (np.std(rmses) / np.mean(rmses) * 100),  # å˜å¼‚ç³»æ•°
        "runs": rmses
    }

    # æ‰“å°ç»Ÿè®¡æ‘˜è¦
    print("\n=== Statistical Summary ===")
    print(f"Mean Â± Std: {stats['mean']:.4f} Â± {stats['std']:.4f}")
    print(f"Range: [{stats['min']:.4f}, {stats['max']:.4f}]")
    print(f"Coefficient of Variation: {stats['cv']:.2f}%")


def print_args(args):
    print("è¿è¡Œå‚æ•°é…ç½®ï¼š")
    print("-" * 40)
    for arg_name, arg_value in vars(args).items():
        print(f"{arg_name:20}: {arg_value}")
    print("-" * 40)


def plow(history, use_adversarial=True):
    if use_adversarial:
        plt.figure(figsize=(15, 12))
        # 1. æ€»æŸå¤±å›¾
        plt.subplot(3, 1, 1)
        plt.plot(history['total'], color='blue')
        plt.title('Total Loss')
        plt.xlabel('Epoch')
        # 2. ä¸»ä»»åŠ¡æŸå¤±å›¾
        plt.subplot(3, 1, 2)
        plt.plot(history['y'], color='green', label='y')
        plt.legend()
        plt.title('Main Task Loss')
        plt.xlabel('Epoch')
        # 3. å¯¹æŠ—æŸå¤±å›¾
        plt.subplot(3, 1, 3)
        plt.plot(history['x'], color='red')
        plt.title('Adversarial Loss')
        plt.xlabel('Epoch')
        plt.tight_layout()
        plt.show()
    else:
        plt.figure(figsize=(15, 12))
        # ä¸»ä»»åŠ¡æŸå¤±å›¾
        plt.plot(history['y'], color='green', label='y')
        plt.legend()
        plt.title('Main Task Loss')
        plt.xlabel('Epoch')
        plt.tight_layout()
        plt.show()


def load_data(data_path):
    return pd.read_csv(data_path, encoding='utf-8')


def _to_tensor(data: Optional[np.ndarray]) -> Optional[torch.Tensor]:
    """å°†numpyæ•°ç»„è½¬æ¢ä¸ºtensorï¼Œå¤„ç†Noneæƒ…å†µ"""
    if data is None:
        return None
    return torch.from_numpy(data).float().to(device)

class WeatherScaler:
    """
    å¤©æ°”æ•°æ®ä¸“ç”¨æ ‡å‡†åŒ–å™¨ï¼Œä¸é‡‘èæ•°æ®ä¿æŒå®Œå…¨ç›¸åŒçš„æ¥å£
    æ”¯æŒä¸‰ç±»ç‰¹å¾çš„æ ‡å‡†åŒ–å’Œåæ ‡å‡†åŒ–
    """

    def __init__(self):
        self.past_scaler = StandardScaler()  # è¿‡å»ç‰¹å¾æ ‡å‡†åŒ–å™¨
        self.forward_scaler = StandardScaler()  # æœªæ¥ç‰¹å¾æ ‡å‡†åŒ–å™¨
        self.target_scaler = StandardScaler()  # ç›®æ ‡å€¼æ ‡å‡†åŒ–å™¨
        self.fitted = False
        self.past_features = None
        self.forward_features = None
        self.target_feature = None
        self.past_mean_ = None
        self.past_std_ = None
        self.forward_mean_ = None
        self.forward_std_ = None
        self.target_mean_ = None
        self.target_std_ = None

    def fit(self, df: pd.DataFrame,
            past_features: List[str],
            forward_features: List[str],
            target_feature: str) -> None:
        """
        æ‹Ÿåˆæ ‡å‡†åŒ–å™¨
        å‚æ•°:
            df: åŒ…å«æ•°æ®çš„DataFrame
            past_features: è¿‡å»ç‰¹å¾åˆ—ååˆ—è¡¨
            forward_features: æœªæ¥ç‰¹å¾åˆ—ååˆ—è¡¨
            target_feature: ç›®æ ‡ç‰¹å¾åˆ—å
        """
        # è¿‡å»ç‰¹å¾æ ‡å‡†åŒ–
        if past_features:
            self.past_scaler.fit(df[past_features])
            self.past_mean_ = self.past_scaler.mean_.copy()
            self.past_std_ = self.past_scaler.scale_.copy()

        # æœªæ¥ç‰¹å¾æ ‡å‡†åŒ–
        if forward_features:
            self.forward_scaler.fit(df[forward_features])
            self.forward_mean_ = self.forward_scaler.mean_.copy()
            self.forward_std_ = self.forward_scaler.scale_.copy()

        # ç›®æ ‡å€¼æ ‡å‡†åŒ–
        self.target_scaler.fit(df[[target_feature]])
        self.target_mean_ = self.target_scaler.mean_.copy()
        self.target_std_ = self.target_scaler.scale_.copy()

        self.past_features = past_features
        self.forward_features = forward_features
        self.target_feature = target_feature
        self.fitted = True

    def transform_past(self, df: pd.DataFrame) -> Optional[np.ndarray]:
        """æ ‡å‡†åŒ–è¿‡å»ç‰¹å¾"""
        assert self.fitted, "è¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•"
        if not self.past_features:
            return None
        return self.past_scaler.transform(df[self.past_features]).astype('float32')

    def transform_forward(self, df: pd.DataFrame) -> Optional[np.ndarray]:
        """æ ‡å‡†åŒ–æœªæ¥ç‰¹å¾"""
        assert self.fitted, "è¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•"
        if not self.forward_features:
            return None
        return self.forward_scaler.transform(df[self.forward_features]).astype('float32')

    def transform_target(self, df: pd.DataFrame) -> np.ndarray:
        """æ ‡å‡†åŒ–ç›®æ ‡å€¼"""
        assert self.fitted, "è¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•"
        return self.target_scaler.transform(df[[self.target_feature]]).astype('float32')

    def inverse_transform_target(self, y: np.ndarray) -> np.ndarray:
        """åæ ‡å‡†åŒ–ç›®æ ‡å€¼"""
        assert self.fitted, "è¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•"
        return self.target_scaler.inverse_transform(y.reshape(-1, 1))

    def inverse_transform_past(self, X_past: np.ndarray) -> Optional[np.ndarray]:
        """åæ ‡å‡†åŒ–è¿‡å»ç‰¹å¾"""
        assert self.fitted, "è¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•"
        if not self.past_features or X_past is None:
            return None
        return self.past_scaler.inverse_transform(X_past)

    def inverse_transform_forward(self, X_forward: np.ndarray) -> Optional[np.ndarray]:
        """åæ ‡å‡†åŒ–æœªæ¥ç‰¹å¾"""
        assert self.fitted, "è¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•"
        if not self.forward_features or X_forward is None:
            return None
        return self.forward_scaler.inverse_transform(X_forward)

    def get_target_stats(self) -> dict:
        """è·å–ç›®æ ‡ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯"""
        assert self.fitted, "è¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•"
        return {
            'mean': self.target_mean_[0],
            'std': self.target_std_[0]
        }

    def get_past_stats(self) -> dict:
        """è·å–è¿‡å»ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯"""
        assert self.fitted, "è¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•"
        if self.past_features is None:
            return {}
        return {
            'mean': dict(zip(self.past_features, self.past_mean_)),
            'std': dict(zip(self.past_features, self.past_std_))
        }

    def get_forward_stats(self) -> dict:
        """è·å–æœªæ¥ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯"""
        assert self.fitted, "è¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•"
        if self.forward_features is None:
            return {}
        return {
            'mean': dict(zip(self.forward_features, self.forward_mean_)),
            'std': dict(zip(self.forward_features, self.forward_std_))
        }
            
def prepare_weather_data(
        data: pd.DataFrame,
        features: Dict[str, List[str]],
        target: str,
        sequence_length: int = 24,
        step_forward: int = 2,
        train_rate: float = 0.7,
        valid_rate: float = 0.2,
        is_scaler: bool = True,
        use_cache: bool = True,
        is_save_cache: bool = True,
        cache_dir: str = './data_cache',
        modelName: str = 'LSTM'
) -> Tuple[List, List, List, TeamStandardScaler, None, None]:
    """
    å¤©æ°”æ•°æ®ä¸“ç”¨é¢„å¤„ç†å‡½æ•°
    é€‚é…å•ä¸€æ—¶é—´åºåˆ—ï¼Œæ¯10åˆ†é’Ÿè®°å½•ä¸€æ¬¡
    """
    # --- 0. ç¼“å­˜æ£€æŸ¥ ---
    if use_cache:
        suffix = "_scaler" if is_scaler else ""
        cache_file = f'{cache_dir}/weather{suffix}.pkl'
        
        print(f"cache_file: {cache_file}")
        
        if os.path.exists(cache_file):
            print(f"âš¡ [Cache] Loading data from: {cache_file}")
            try:
                return joblib.load(cache_file)
            except Exception:
                print("Cache load failed, reprocessing...")
                os.makedirs(cache_dir, exist_ok=True)

    print("ğŸš€ Starting Weather Data Preprocessing...")
    
    # --- 1. æ—¥æœŸå¤„ç† ---
    # åˆ é™¤ç©ºå€¼
    data = data.dropna(subset=['date', target])
    
    # è½¬æ¢ä¸ºdatetime (å‡è®¾æ¯10åˆ†é’Ÿè®°å½•ï¼Œæ ¼å¼çµæ´»å¤„ç†)
    try:
        # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
        try:
            data['date_obj'] = pd.to_datetime(data['date'], format='%Y%m%d%H%M')
        except:
            try:
                data['date_obj'] = pd.to_datetime(data['date'])
            except:
                # å¦‚æœéƒ½ä¸è¡Œï¼Œå°è¯•è½¬ä¸ºå­—ç¬¦ä¸²å†è§£æ
                data['date_obj'] = pd.to_datetime(data['date'].astype(str))
    except Exception as e:
        print(f"æ—¥æœŸè½¬æ¢å¤±è´¥ï¼Œè¯·æ£€æŸ¥ date åˆ—æ•°æ®æ ¼å¼ã€‚ç¤ºä¾‹æ•°æ®: {data['date'].iloc[0]}")
        raise e
    
    # æ·»åŠ æ—¶é—´ç‰¹å¾
    if 'year' in features['forward_feature']:
        data['year'] = data['date_obj'].dt.year
    if 'month' in features['forward_feature']:
        data['month'] = data['date_obj'].dt.month
    if 'hour' in features['forward_feature']:
        data['hour'] = data['date_obj'].dt.hour
    if 'minute' in features['forward_feature']:
        data['minute'] = data['date_obj'].dt.minute
    
    # --- 2. ç‰¹å¾ç­›é€‰ä¸ç¼ºå¤±å€¼å¤„ç† ---
    required_cols = list(set(
        ['date_obj', target] + 
        features['past_feature'] + 
        features['forward_feature']
    ))
    
    data = data[required_cols]
    target_related_cols = [target]
    
    # ä½¿ç”¨å¤©æ°”ä¸“ç”¨ç¼ºå¤±å€¼å¤„ç†
    data, updated_past, updated_forward = handle_missing_values_weather(
        data, 
        features, 
        target_cols=target_related_cols,
        threshold=0.4
    )
    
    features['past_feature'] = updated_past
    features['forward_feature'] = updated_forward

    # === [æ–°å¢å…³é”®é€»è¾‘] ç‰¹å¾é‡æ’: å¼ºåˆ¶ Target åˆ°æœ€å ===
    # ç¡®ä¿ Target å­˜åœ¨äº past_features ä¸­ (å¦‚æœæ˜¯è‡ªå›å½’é¢„æµ‹ï¼Œé€šå¸¸éƒ½åœ¨)
    if modelName == 'TimeMixer':
        if target in features['past_feature']:
            # å…ˆç§»é™¤ï¼Œå†è¿½åŠ åˆ°æœ«å°¾
            features['past_feature'].remove(target)
            features['past_feature'].append(target)
        else:
            # å¦‚æœ Target ä¸åœ¨ç‰¹å¾é‡Œ (æå…¶ç½•è§)ï¼Œå¿…é¡»åŠ è¿›å»ï¼Œå¦åˆ™æ— æ³•åˆ©ç”¨è‡ªå›å½’è¶‹åŠ¿
            print(f"âš ï¸ è­¦å‘Š: Target '{target}' ä¸åœ¨å†å²ç‰¹å¾ä¸­ï¼Œå¼ºåˆ¶æ·»åŠ ä»¥é€‚é… TimeMixer")
            features['past_feature'].append(target)
            
        print(f"âš¡ [Feature Alignment] ç‰¹å¾é¡ºåºå·²é‡æ’ï¼ŒTarget '{target}' ä½äºæœ€åä¸€ä½ã€‚")
        print(f"âš¡ Past Features: {features['past_feature']}")
    
    
    # ç¡®ä¿æŒ‰æ—¶é—´æ’åº
    data = data.sort_values('date_obj')
    
    # --- 3. æ•°æ®é›†åˆ‡åˆ† (æŒ‰æ—¶é—´) ---
    unique_dates = sorted(data['date_obj'].unique())
    train_split = int(len(unique_dates) * train_rate)
    valid_split = train_split + int(len(unique_dates) * valid_rate)

    train_dates = unique_dates[:train_split]
    valid_dates = unique_dates[train_split - sequence_length : valid_split]  # é¢„ç•™ overlap
    test_dates = unique_dates[valid_split - sequence_length :]

    train_data = data[data['date_obj'].isin(train_dates)].copy()
    valid_data = data[data['date_obj'].isin(valid_dates)].copy()
    test_data = data[data['date_obj'].isin(test_dates)].copy()
    
    print(f"æ•°æ®é›†åˆ’åˆ†: è®­ç»ƒé›† {len(train_data)} è¡Œ, éªŒè¯é›† {len(valid_data)} è¡Œ, æµ‹è¯•é›† {len(test_data)} è¡Œ")
    
    # --- 4. ç»Ÿè®¡åˆ†æ ---
    # è¿™é‡Œå¯ä»¥è°ƒç”¨æ‚¨çš„ç»Ÿè®¡åˆ†æå‡½æ•°
    # stats_results = analyze_data_distribution(
    #     train_data, valid_data, test_data, 
    #     target, features
    # )
    # print(stats_results)
    
    # --- 5. æ ‡å‡†åŒ– ---
    # å¤©æ°”æ•°æ®ä¸éœ€è¦LabelEncoderï¼Œä½†ä¸ºäº†ä¿æŒæ¥å£ä¸€è‡´ï¼Œåˆ›å»ºè™šæ‹Ÿç¼–ç å™¨
    # from sklearn.preprocessing import StandardScaler
    # import numpy as np
    
    scaler = WeatherScaler()
    scaler.fit(
        train_data,
        past_features=features['past_feature'],
        forward_features=features['forward_feature'],
        target_feature=target
    )

    # === [ä¿®æ”¹ç‚¹ A] ç”Ÿæˆå…¨é‡æ—¶é—´ç‰¹å¾ ===
    print("âš¡ [Adapter] Generating Time Features")
    global_time_marks = time_features(data['date_obj'])
    
    # --- 6. é«˜é€Ÿåºåˆ—ç”Ÿæˆ (å¤©æ°”ä¸“ç”¨) ---
    def create_weather_sequences(subset_data):
        """å¤©æ°”æ•°æ®ä¸“ç”¨åºåˆ—ç”Ÿæˆï¼Œå•ä¸€æ—¶é—´åºåˆ—"""
        sequences = []
        has_forward = len(features['forward_feature']) > 0
        
        # ç¡®ä¿æŒ‰æ—¶é—´æ’åº
        subset_data = subset_data.sort_values('date_obj')
        
        # æå–æ—¶é—´ç´¢å¼•ï¼ˆè¿ç»­çš„æ—¶é—´ç‚¹ï¼‰
        dates = subset_data['date_obj'].values
        time_indices = np.arange(len(subset_data))  # ä½¿ç”¨ç®€å•æ•´æ•°ç´¢å¼•

        # === [ä¿®æ”¹ç‚¹ B] è·å–å½“å‰å­é›†å¯¹åº”çš„æ—¶é—´ç‰¹å¾ ===
        # ç®€å•é«˜æ•ˆçš„åšæ³•ï¼šç›´æ¥å¯¹å½“å‰å­é›†çš„æ—¶é—´åˆ—å†ç®—ä¸€æ¬¡
        subset_time_marks = time_features(subset_data['date_obj'])
        
        # ç‰¹å¾ä¸ç›®æ ‡
        scaled_past = scaler.transform_past(subset_data)
        scaled_forward = scaler.transform_forward(subset_data) if has_forward else None
        
        if is_scaler:
            scaled_target = scaler.transform_target(subset_data)
        else:
            scaled_target = subset_data[[target]].values.astype('float32')
        
        num_samples = len(subset_data)
        max_idx = num_samples - sequence_length - step_forward + 1
        
        if max_idx <= 0:
            print(f"è­¦å‘Š: æ•°æ®é•¿åº¦ {num_samples} ä¸è¶³ä»¥ç”Ÿæˆåºåˆ—")
            return sequences
        
        # æ„å»ºåºåˆ—
        for i in range(max_idx):
            # ç´¢å¼•å®šä¹‰
            idx_input_start = i
            idx_input_end = i + sequence_length
            idx_target_start = idx_input_end
            idx_target_end = idx_input_end + step_forward
            
            # æ£€æŸ¥è¿ç»­æ€§ï¼ˆå¤©æ°”æ•°æ®æ¯10åˆ†é’Ÿä¸€æ¬¡ï¼‰
            # ä½¿ç”¨æ—¶é—´ç´¢å¼•æ£€æŸ¥ï¼Œç¡®ä¿æ˜¯è¿ç»­çš„10åˆ†é’Ÿé—´éš”
            time_gap_input = time_indices[idx_input_end-1] - time_indices[idx_input_start]
            time_gap_target = time_indices[idx_target_start] - time_indices[idx_input_end-1]
            
            # è¾“å…¥çª—å£å†…éƒ¨åº”è¯¥è¿ç»­
            if time_gap_input != sequence_length - 1:
                continue
            # é¢„æµ‹ç›®æ ‡ç´§æ¥è¾“å…¥çª—å£
            if time_gap_target != 1:
                continue
            # é¢„æµ‹ç›®æ ‡çª—å£å†…éƒ¨è¿ç»­
            if step_forward > 1:
                if time_indices[idx_target_end-1] - time_indices[idx_target_start] != step_forward - 1:
                    continue

            # === [ä¿®æ”¹ç‚¹ C] æå–å½“å‰çª—å£çš„æ—¶é—´ç‰¹å¾ (è¦†ç›– Input + Pred) ===
            # TimeMixer éœ€è¦çš„æ—¶é—´ç‰¹å¾é•¿åº¦ = seq_len + pred_len (ç”¨äº Encoder å’Œ Decoder)
            # å¯¹åº”çš„ç´¢å¼•èŒƒå›´æ˜¯ [idx_input_start : idx_target_end]
            current_time_mark = subset_time_marks[idx_input_start : idx_target_end]
            
            sequences.append((
                scaled_past[idx_input_start:idx_input_end] if scaled_past is not None else None,
                scaled_forward[idx_input_start:idx_target_end] if scaled_forward is not None else None,
                scaled_target[idx_input_start:idx_target_end],  # åªå–é¢„æµ‹æœŸçš„ç›®æ ‡
                dates[idx_input_end - 1],  # å‚è€ƒæ—¥æœŸ
                0,  # è™šæ‹Ÿå®ä½“ID
                "weather_station",  # è™šæ‹Ÿæ ‡è¯†
                subset_data[[target]].values[idx_input_start:idx_target_end],  # åŸå§‹å€¼
                current_time_mark  # <--- [æ–°å¢] ç¬¬8ä¸ªå…ƒç´ ï¼šæ—¶é—´ç‰¹å¾
            ))
        
        return sequences
    
    print("âš¡ ç”Ÿæˆå¤©æ°”æ•°æ®åºåˆ—...")
    train_seq = create_weather_sequences(train_data)
    valid_seq = create_weather_sequences(valid_data)
    test_seq = create_weather_sequences(test_data)
    
    print(f"âœ” æ•°æ®å‡†å¤‡å®Œæˆ: è®­ç»ƒé›†={len(train_seq)} åºåˆ—, éªŒè¯é›†={len(valid_seq)} åºåˆ—, æµ‹è¯•é›†={len(test_seq)} åºåˆ—")
    
    result = (train_seq, valid_seq, test_seq, scaler, None, None)
    
    # --- 7. ä¿å­˜ç¼“å­˜ ---

    if is_save_cache:
        os.makedirs(cache_dir, exist_ok=True)
        suffix = "_scaler" if is_scaler else ""
        cache_file = f'{cache_dir}/weather{suffix}.pkl'
        
        print(f"ğŸ’¾ Saving cache to {cache_file}")
        joblib.dump(result, cache_file)
    
    return result

def handle_missing_values_weather(data, features, target_cols, threshold=0.4):
    """
    å¤©æ°”æ•°æ®ä¸“ç”¨ç¼ºå¤±å€¼å¤„ç†å‡½æ•°
    ç®€åŒ–ç‰ˆï¼šåªå¤„ç†å•ä¸€åºåˆ—ï¼Œæ— éœ€æˆªé¢æ“ä½œ
    """
    import numpy as np
    
    # åˆ›å»ºæ·±æ‹·è´
    data = data.copy()
    print(f"å¼€å§‹ç¼ºå¤±å€¼å¤„ç†... åˆå§‹ç»´åº¦: {data.shape}")
    
    # 1. å…¨å±€ Inf æ¸…æ´—
    data = data.replace([np.inf, -np.inf], np.nan)
    
    # 2. åˆ—ç­›é€‰
    keep_always = set(['date_obj'] + target_cols)
    missing_ratio = data.isnull().mean()
    
    cols_to_drop = [
        col for col in data.columns 
        if col not in keep_always and missing_ratio[col] > threshold
    ]
    
    if cols_to_drop:
        print(f"åˆ é™¤é«˜ç¼ºå¤±ç‡åˆ— (> {threshold:.0%}): {cols_to_drop}")
        data = data.drop(columns=cols_to_drop)
    
    # æ›´æ–°ç‰¹å¾åˆ—è¡¨
    new_past_features = [f for f in features['past_feature'] if f not in cols_to_drop]
    new_forward_features = [f for f in features['forward_feature'] if f not in cols_to_drop]
    
    # 3. å¼ºåˆ¶ç±»å‹æ¸…æ´—
    data = data.sort_values('date_obj')
    processing_cols = [c for c in data.columns if c != 'date_obj']
    
    print("æ­£åœ¨è¿›è¡Œæ·±åº¦ç±»å‹æ¸…æ´—ä¸å¡«å……...")
    for col in processing_cols:
        # å¼ºåˆ¶è½¬æ•°å€¼
        data[col] = pd.to_numeric(data[col], errors='coerce')
        
        # å†æ¬¡æ£€æŸ¥ Inf
        mask_inf = np.isinf(data[col])
        if mask_inf.any():
            data.loc[mask_inf, col] = np.nan

        # å¦‚æœè¯¥åˆ—å…¨æ˜¯ NaNï¼Œç›´æ¥å¡« 0
        if data[col].isnull().all():
            print(f"è­¦å‘Š: åˆ— {col} å…¨æ˜¯ NaNï¼Œå·²å¡«å……ä¸º 0")
            data[col] = 0.0
            continue

        if data[col].isnull().sum() == 0:
            continue
            
        # å¤©æ°”æ•°æ®ä¸“ç”¨å¡«å……ï¼šä»…ä½¿ç”¨æ—¶é—´åºåˆ—å¡«å……
        try:
            # çºµå‘å¡«å……: å‰3æœŸç§»åŠ¨å¹³å‡
            data[col] = data[col].fillna(
                data[col].shift(1).rolling(window=3, min_periods=1).mean()
            )
            '''
            # å¦‚æœè¿˜æœ‰ç¼ºå¤±ï¼Œç”¨å‰åå‡å€¼å¡«å……
            if data[col].isnull().sum() > 0:
                data[col] = data[col].fillna(method='ffill').fillna(method='bfill')
            '''    
            # å…œåº•å¡«å……: å…¨å±€ 0 å€¼
            if data[col].isnull().sum() > 0:
                data[col] = data[col].fillna(0.0)
                
        except Exception as e:
            print(f"åˆ— {col} å¡«å……å¤±è´¥: {e}, å¼ºåˆ¶å¡«å…… 0")
            data[col] = data[col].fillna(0.0)

    # 4. æœ€ç»ˆæ ¸æŸ¥
    if np.isinf(data[processing_cols]).values.any():
        print("!!! è­¦å‘Š: æ•°æ®ä¸­ä»å­˜åœ¨ infï¼Œå¼ºåˆ¶æ›¿æ¢ä¸º 0 !!!")
        data = data.replace([np.inf, -np.inf], 0.0)
        
    before_len = len(data)
    data = data.dropna()
    after_len = len(data)
    
    if before_len != after_len:
        print(f"æœ€ç»ˆæ¸…æ´—åˆ é™¤äº† {before_len - after_len} è¡Œ")
        
    print(f"å¤„ç†å®Œæˆ. ç»´åº¦: {data.shape}")
    
    return data, new_past_features, new_forward_features